TO DO:

-   Fix unit tests

-

    Dual-layer regime cells -- almost done

    Interval propagation (client + account)

    Header auto-updates with intervals

-

    Interval selector UI (page-wide)

    Persist interval per client/account

    CAPM period auto-sync (1Y to 252, etc)

-

    Lot-aware cost basis (parallel to holdings)

    Weighted average cost display

    Exposure-at-risk metrics

-

    Regime-conditioned exposure

    Regime-specific drawdowns

    Stress testing via regime shocks


You are an expert Python engineer + quant-minded financial systems architect. You are working inside an existing repo. Your job is to do a deep dive, produce a concrete implementation plan, and then implement changes with comprehensive tests.

Non-negotiables:
- Every calculation in the repo must have unit tests. No exceptions.
- Any change you make must be covered by tests (unit tests at minimum; integration tests where appropriate).
- If you find any missing tests for existing calculations, add them.
- Accuracy is mandatory: for each calculation, write benchmark/known-value tests (golden cases) and edge cases.
- Maintain modularity and future-proofing. If a subsystem is not scalable, refactor it.
- Avoid “quick patches.” Fix root causes and clarify interfaces.
- Do not remove features. Deprecate with clear migration only if strictly necessary.
- All menus should be moved to the new modular menu used in main menu (the one that highlights current selection, etc)
- Do not change format unless absolutely necessary and you must show an example/ask before changing it.
- The above should only be done for optimization or if a current set of info is dispalyed totally incorrectly or inefficiently.

Deliverables (in this order):
1) Repo audit summary (what exists, what is missing, what is broken).
2) Detailed plan broken into phases with acceptance criteria per phase.
3) Implementation changes across the codebase (not stubs), including:
   - Holdings/lots/cost-basis capability completed end-to-end
   - Financial toolkit context upgrades (rename + explanatory text)
   - Test suite expansion and extension (continue existing suite)
   - Local model (offline) report generation architecture + install path
4) All tests passing; add/extend CI instructions if present.

You must be extremely specific. Make no assumptions without verifying in the codebase.

========================================================
PART 0 — INITIAL ORIENTATION (MANDATORY)
========================================================
- Print the full repo tree and identify core entrypoints (e.g., run.py, main.py, cli entry, etc.).
- Identify modules for:
  - clients / accounts / holdings / lots
  - market data (pricing, history)
  - persistence (JSON/CSV/SQLite/etc.)
  - reporting
  - “financial toolkit” (calculations)
  - test framework and existing tests
- Run the current test suite and report failures, warnings, skipped tests, and coverage if available.
- Execute the primary app flow (or simulate via unit tests) to understand current UX for:
  - creating clients
  - adding accounts/sub-accounts
  - adding holdings
  - viewing holdings
  - generating any reports

Output: a concise audit with file paths, what each area does, and which parts require refactor.

========================================================
PART 1 — HOLDINGS: COMPLETE COST BASIS + HISTORICAL PURCHASE SUPPORT
========================================================
Problem statement:
This codebase still lacks full ability to add CURRENT holdings per account that were bought in the past at a certain cost basis. The repo started this but it must be implemented fully. Many brokerage apps provide limited info (sometimes only total shares and average cost; sometimes only total cost; sometimes per-lot detail like Webull). The system must support partial information while preserving correctness.

Your task:
Implement a robust holdings + lots model that supports:
- Adding a holding with:
  A) Shares + average cost basis (common case)
  B) Shares + total cost (derive avg cost)
  C) Lot-level entries (timestamped) with per-lot shares + cost basis
  D) Mixed cases: user knows some lots precisely and the rest only as an aggregate average
- Timestamp support MUST include date+time, not just date.
- Historical price linking:
  - If user provides timestamp only (and not cost), fetch historical price nearest (or at) timestamp and compute cost basis.
  - If user provides cost basis explicitly, do not overwrite it with “current price.”
  - The UI must clearly differentiate:
    - current market price
    - average cost basis (position-level)
    - per-lot cost basis
- Rendering:
  - In account holdings display, lot rows must appear directly under their parent ticker row.
  - Parent ticker row “Price/Share” column must show average cost basis (not current price).
  - Lot rows “Price/Share” column must show that lot’s cost basis.
  - Current market price should be displayed in a separate column (e.g., “Market Price”) to avoid confusion.

Data model requirements:
- Define (or confirm) canonical types:
  - Client
  - Account (and sub-accounts if present)
  - HoldingPosition (per ticker/asset)
  - Lot (timestamped acquisition slices)
- Ensure:
  - No duplication of ticker state across lots and position
  - Computed fields are derived deterministically (avg cost = weighted average of lots and/or aggregate entries)
  - All monetary values are stored consistently (currency handling, rounding rules)
  - Timestamps stored in an unambiguous format (ISO 8601 recommended)

Persistence requirements:
- Ensure save/load preserves lots exactly, including timestamps, not overwritten by “now.”
- Implement migration if needed:
  - If old data stored only a single timestamp or “N/A,” convert gracefully
  - Do not break existing user files

Input/UX requirements:
- Provide an “Add/Update Holding” flow that supports:
  1) Market-priced add (current price)
  2) Historical add (timestamp -> historical price)
  3) Manual add (user enters cost basis)
  4) Lot add (repeatable entries)
  5) Aggregate add (avg cost or total cost)
- Validate user input strictly:
  - shares must be positive
  - cost must be non-negative
  - timestamps must parse reliably
  - ticker normalization rules (case, asset types)

Architecture requirements:
- Use the existing “components and modules” format.
- If any current factoring is inefficient for scaling, refactor:
  - Separate domain logic (models, calculations) from I/O (CLI/UI) and from data fetching (API clients).
  - Introduce service interfaces for price history retrieval and current quotes.
  - Add caching where appropriate (but ensure correctness first).

Testing requirements for Part 1 (mandatory):
- Unit tests for:
  - Weighted average cost basis computation across multiple lots
  - Mixed aggregate + lot scenarios
  - Historical price lookup selection logic (nearest, exact, timezone if relevant)
  - Persistence round-trip: create -> save -> load -> equals (deep equality)
  - Rendering ordering: lots appear under parent holding
  - Regression test: “lots showing today’s price” must be prevented
- Include golden tests with known numeric results.
- Include edge cases:
  - single-lot
  - many lots
  - fractional shares
  - very old timestamps
  - missing price history fallback behavior

Acceptance criteria for Part 1:
- A user can enter a holding acquired in 2023-11-10T14:30:00 and it retains that timestamp and its cost basis.
- The table shows parent ticker with avg cost basis, and child lot rows with per-lot cost basis.
- No part of the code overwrites historical timestamps with “now.”
- All tests pass.

========================================================
PART 2 — FINANCIAL TOOLKIT: RENAME + ADD USER-COMPREHENSIBLE CONTEXT
========================================================
Problem statement:
There is nearly zero context on values in the “financial toolkit.” It should likely be renamed to “tools” for menu clarity. Each metric must be understandable to someone with zero context.

Your task:
- Identify every calculation/metric presented in the toolkit (volatility, beta, entropy, Sharpe, drawdown, correlation, VaR, etc., whatever exists).
- For each metric, add:
  - Plain-language definition (1–3 sentences)
  - What “high” vs “low” implies, with practical interpretation
  - Typical ranges / caveats (if meaningful)
  - Units (percentage, ratio, dollars, etc.)
  - Limitations and when it can mislead
- Replace shallow explanations like “high = noisier” with concrete meaning:
  Example requirement for entropy:
  - Define entropy in the context used (return distribution? price changes? signal complexity?)
  - Explain what it measures, what high/low means for investors, and why it matters
  - If entropy implementation is simplistic, note it and propose an improved measure or rename it to avoid misrepresentation

Rename requirements:
- Rename “financial_toolkit” to “tools” (or the best repo-consistent name).
- Update all imports, menus, docs, references.
- Ensure backward compatibility if external references exist (provide alias module or deprecation shim).

Testing requirements for Part 2:
- Every metric must have unit tests:
  - Known inputs -> expected outputs
  - Boundary conditions (empty series, constant series, NaNs)
  - Deterministic rounding
- If metrics depend on external data, isolate logic so tests use fixed fixtures (no network).

Acceptance criteria for Part 2:
- A user with no finance background can read each tool output and understand what it means and how to use it.
- Metrics are tested, deterministic, and documented.

========================================================
PART 3 — TEST SUITE: EXTEND AND ENFORCE
========================================================
You must:
- Use the existing test framework already present (pytest/unittest/etc.).
- Add tests in the same style and folder structure.
- Add coverage for any newly introduced modules AND any existing calculation lacking tests.
- Add a test “gate” conceptually:
  - If a calculation exists, it must be referenced by at least one test file.
  - If feasible, add a simple coverage report instruction in docs/CI.

Additionally:
- Identify flaky tests or tests that depend on live APIs; refactor to use mocks/fixtures.
- Ensure the suite runs fast (unit tests should be sub-second to a few seconds; integration can be separated).

Acceptance criteria for Part 3:
- “pytest” (or the repo’s test runner) passes locally.
- No tests call the network by default.

========================================================
PART 4 — LOCAL MODEL (OFFLINE) REPORT GENERATION: DETAILED PLAN + IMPLEMENTATION START
========================================================
Problem statement:
The repo has started “free local model use” but it must be made production-grade to generate dynamic client reports pulling from news, conflict, plane/ship, and all other data modules. Reports must aggregate as much data as is safe. Installation must be easy (requirements.txt, or cross-platform manager like Scoop for Windows, etc.). The model strategy must be modular. Accuracy remains non-negotiable: calculations used in reports must be benchmarked and unit-tested.

Your task:
A) Architecture plan (must be extremely detailed)
- Identify current report generation pipeline: where “reports” exist, how they are triggered, and where data comes from.
- Propose a modular “Report Engine” with:
  - Data sources (news, market, conflict, aviation, maritime, etc.) as pluggable providers
  - Normalization layer: canonical schema for “events,” “entities,” “exposures,” “risks”
  - Prompt builder: deterministic template assembly with citations/attribution when data is external
  - Model runner abstraction:
    - local LLM via Ollama (or equivalent) as a backend
    - fallback modes (no model installed -> degrade gracefully with a templated report)
  - Output formats:
    - terminal view
    - markdown
    - JSON
    - optionally PDF later (design for it)
- Include security/privacy considerations:
  - avoid sending data externally
  - redact sensitive client identifiers in prompts by default (configurable)

B) Installation and dependency plan (cross-platform)
- Determine repo’s install flow:
  - requirements.txt / poetry / pip-tools / etc.
  - run.py bootstrapping behaviors
- Implement a frictionless setup path for local models:
  - Detect whether Ollama (or chosen runtime) is installed
  - Provide OS-specific instructions in code and docs
  - On Windows: optionally Scoop install path if repo already uses Scoop patterns
  - On macOS: brew
  - On Linux: official install or package manager
- Add a “health check” command:
  - verifies model runtime reachable
  - verifies at least one configured model present (e.g., llama3, mistral, etc.)
  - verifies report pipeline dependencies

C) Implementation requirements (initial milestone)
- Implement the Report Engine skeleton + at least one end-to-end report:
  - Select one report type (e.g., “Client Weekly Brief”)
  - Pull deterministic data from existing modules using mockable interfaces
  - Generate a report in markdown with sections:
    - Portfolio snapshot (cost basis, market value, P/L if available)
    - News relevant to held tickers/entities
    - Risk notes (vol, concentration, correlation if those tools exist)
    - Conflict/geo notes if those modules exist and are relevant
    - Aviation/maritime notes if relevant modules exist
    - “Data freshness” and “Methodology” footer
- The LLM output must be constrained:
  - Use structured prompting that requests JSON first (if feasible), then render into markdown
  - Validate JSON schema (unit test it)
  - If model returns invalid schema, retry with a repair prompt or fall back to deterministic template

D) Testing requirements for Part 4
- Unit tests for:
  - prompt builder (given fixtures -> exact prompt string or structured prompt object)
  - schema validation for model outputs
  - report renderer (fixtures -> stable markdown)
  - health check behaviors (installed vs not installed mocked)
- Integration tests (optional but preferred):
  - run report generation in “no-model” mode and confirm output sections exist
  - run report generation in “mock model” mode

Acceptance criteria for Part 4:
- A user can run a single command to generate a report without installing a model (template mode).
- If the model is installed, the report is enhanced, but still validated and deterministic in structure.
- No network calls required for baseline report generation unless explicitly enabled.

========================================================
PART 5 — ADDITIONAL ISSUES: NOTE THEM AND FIX IF IN SCOPE
========================================================
During your deep dive, if you see ANY other issues (bugs, design flaws, misleading naming, unstable modules, poor factoring, code duplication, timestamp handling mistakes, I/O intertwined with domain logic), you must:
- List them in the audit
- Propose fix priority (P0, P1, P2)
- Fix P0 issues immediately if they block correctness or tests
- Ensure any fix has tests

========================================================
OUTPUT FORMAT REQUIREMENTS (VERY IMPORTANT)
========================================================
When reporting back, you must include:
- File paths for changes
- “Before/after” behavior description
- Exact test commands to run
- A short migration note if persistence format changed
- A checklist mapping each acceptance criterion to proof (tests, screenshots/log output, etc.)

Proceed now:
1) Run the orientation steps.
2) Produce the audit and plan.
3) Implement Part 1 fully with tests.
4) Implement Part 2 fully with tests.
5) Ensure Part 3 test suite is extended and green.
6) Implement Part 4 milestone (engine + one report + health check) with tests.
7) Summarize additional issues and recommended next steps.
8) These next steps should include finding potentially a more reliable and widely used flight tracking and shipping tracking data source than OpenSky.
9) Plan fixing the fact that not all news sources (RSS etc) are loading or being reported on and new is not refreshing. It also it not always relevant but the AI model usage should fix this.

========================================================
UNFINISHED COMPONENTS PLAN — MULTI-UI DATA LAYER
========================================================
Goal: single canonical data model + service layer powering CLI/Web/Mobile with consistent formatting, validation, and calculations.

Phase A — Domain & Schema Hardening (P0)
- Define canonical domain schemas (Client, Account, HoldingPosition, Lot, NewsItem, Report, Metric).
- Add explicit versioning for persisted data (e.g., schema_version in clients.json/report_cache).
- Build a schema registry (module-level map) with validators and upgrade/migration functions per version.
- Acceptance: schema validation runs on startup; malformed payloads report actionable errors; migrations are deterministic and tested.

Phase B — Storage Abstraction (P0)
- Introduce repository interfaces for persistence (ClientRepository, NewsRepository, ReportRepository).
- Provide FileRepository implementations wrapping existing JSON files.
- Add a persistence config (settings) to select backend (file, sqlite future).
- Acceptance: CLI uses repositories only; file paths become implementation detail.

Phase C — Service Layer & DTOs (P1)
- Create service layer (ClientService, PortfolioService, NewsService, ReportService).
- Add input/output DTOs for UI boundaries (CLI/UI should not touch raw storage models).
- Add deterministic formatting adapters (table rows, markdown sections) that operate on DTOs.
- Acceptance: CLI calls only services; data formatting for CLI is isolated from data storage.

Phase D — Event + Cache Strategy (P1)
- Add in-process event bus or hooks (data_changed events) for cache invalidation.
- Add per-service cache with TTL and explicit invalidation (especially for news and prices).
- Acceptance: news freshness is centralized and does not require per-module updates.

Phase E — API Readiness (P2)
- Add API layer stubs (FastAPI/Flask) using service layer + DTOs.
- Ensure deterministic outputs for API consumers (JSON schema).
- Acceptance: API endpoints are thin wrappers over service calls, no business logic.

Phase F — Tests (P0/P1)
- Add schema validation tests per entity, migration tests per version.
- Add repository contract tests (file-based + future backends).
- Add service integration tests with mocked repositories.
- Ensure no network by default; fixtures for price/news.

Notes:
- Keep calculations in domain modules (toolkit/valuation/holdings) and call from services.
- Preserve existing JSON format; add migration adapters rather than breaking changes.
- Ensure report engine uses DTOs + service outputs, not file IO.


========================================================
NEXT NOTE — ENTROPY VARIANTS
========================================================
Current entropy metric is histogram-based Shannon entropy of returns.
Consider adding permutation entropy (ordinal pattern entropy) as a separate metric
for time-order complexity, and document both in the toolkit glossary and README.
